use std::collections::HashMap;

use crate::game::{Game, Player};

/// Calculates the exploitability of a strategy.
///
/// Exploitability measures how far a strategy is from Nash equilibrium.
/// It's defined as the average of the best response values for both players.
/// A Nash equilibrium has exploitability of 0.
#[must_use]
#[allow(clippy::implicit_hasher)]
pub fn calculate_exploitability<G: Game>(game: &G, strategy: &HashMap<String, Vec<f64>>) -> f64 {
    let br_value_p1 = best_response_value(game, strategy, Player::Player1);
    let br_value_p2 = best_response_value(game, strategy, Player::Player2);

    f64::midpoint(br_value_p1, br_value_p2)
}

/// Computes the expected value for a player using best response against opponent's strategy.
///
/// This implementation first computes counterfactual values for each action at each info set,
/// then determines the best response action, and finally computes the expected value.
fn best_response_value<G: Game>(
    game: &G,
    strategy: &HashMap<String, Vec<f64>>,
    player: Player,
) -> f64 {
    // First pass: compute counterfactual values for each action at each info set
    let mut info_set_values: HashMap<String, Vec<f64>> = HashMap::new();
    let mut info_set_reach: HashMap<String, f64> = HashMap::new();

    let initial_states = game.initial_states();

    for state in &initial_states {
        compute_counterfactual_values(
            game,
            strategy,
            state,
            player,
            1.0,
            &mut info_set_values,
            &mut info_set_reach,
        );
    }

    // Determine best response action at each info set
    let mut best_response: HashMap<String, usize> = HashMap::new();
    for (info_set, values) in &info_set_values {
        let best_action = values
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .map_or(0, |(i, _)| i);
        best_response.insert(info_set.clone(), best_action);
    }

    // Second pass: compute expected value using best response
    let mut total_value = 0.0;
    for state in &initial_states {
        total_value += compute_br_value(game, strategy, state, player, 1.0, &best_response);
    }

    // Average over all initial states
    #[allow(clippy::cast_precision_loss)]
    let avg = total_value / initial_states.len() as f64;
    avg
}

/// First pass: compute counterfactual values for each action at BR player's info sets.
#[allow(clippy::too_many_arguments)]
fn compute_counterfactual_values<G: Game>(
    game: &G,
    strategy: &HashMap<String, Vec<f64>>,
    state: &G::State,
    player: Player,
    opponent_reach: f64,
    info_set_values: &mut HashMap<String, Vec<f64>>,
    info_set_reach: &mut HashMap<String, f64>,
) -> f64 {
    if game.is_terminal(state) {
        return game.utility(state, player);
    }

    let current_player = game.player(state);
    let actions = game.actions(state);
    let num_actions = actions.len();
    let info_set = game.info_set_key(state);

    if current_player == player {
        // BR player's node: compute value of each action
        let mut action_values = vec![0.0; num_actions];

        for (i, &action) in actions.iter().enumerate() {
            let next_state = game.next_state(state, action);
            action_values[i] = compute_counterfactual_values(
                game,
                strategy,
                &next_state,
                player,
                opponent_reach,
                info_set_values,
                info_set_reach,
            );
        }

        // Accumulate weighted action values for this info set
        let entry = info_set_values
            .entry(info_set.clone())
            .or_insert_with(|| vec![0.0; num_actions]);
        let reach_entry = info_set_reach.entry(info_set).or_insert(0.0);

        for (i, &v) in action_values.iter().enumerate() {
            entry[i] += opponent_reach * v;
        }
        *reach_entry += opponent_reach;

        // Return max value (for now, we'll refine in second pass)
        action_values
            .iter()
            .copied()
            .fold(f64::NEG_INFINITY, f64::max)
    } else {
        // Opponent's node: play according to strategy
        let opponent_strategy = get_strategy(strategy, &info_set, num_actions);
        let mut value = 0.0;

        for (i, &action) in actions.iter().enumerate() {
            let action_prob = opponent_strategy[i];
            if action_prob > 0.0 {
                let next_state = game.next_state(state, action);
                value += action_prob
                    * compute_counterfactual_values(
                        game,
                        strategy,
                        &next_state,
                        player,
                        opponent_reach * action_prob,
                        info_set_values,
                        info_set_reach,
                    );
            }
        }

        value
    }
}

/// Second pass: compute expected value using determined best response actions.
fn compute_br_value<G: Game>(
    game: &G,
    strategy: &HashMap<String, Vec<f64>>,
    state: &G::State,
    player: Player,
    prob: f64,
    best_response: &HashMap<String, usize>,
) -> f64 {
    if game.is_terminal(state) {
        return prob * game.utility(state, player);
    }

    let current_player = game.player(state);
    let actions = game.actions(state);
    let info_set = game.info_set_key(state);

    if current_player == player {
        // BR player: use best response action
        let best_action_idx = best_response.get(&info_set).copied().unwrap_or(0);
        let action = actions[best_action_idx];
        let next_state = game.next_state(state, action);
        compute_br_value(game, strategy, &next_state, player, prob, best_response)
    } else {
        // Opponent: play according to strategy
        let opponent_strategy = get_strategy(strategy, &info_set, actions.len());
        let mut value = 0.0;

        for (i, &action) in actions.iter().enumerate() {
            let action_prob = opponent_strategy[i];
            if action_prob > 0.0 {
                let next_state = game.next_state(state, action);
                value += compute_br_value(
                    game,
                    strategy,
                    &next_state,
                    player,
                    prob * action_prob,
                    best_response,
                );
            }
        }

        value
    }
}

/// Gets strategy for an info set, defaulting to uniform if not found.
fn get_strategy(
    strategy: &HashMap<String, Vec<f64>>,
    info_set: &str,
    num_actions: usize,
) -> Vec<f64> {
    strategy.get(info_set).cloned().unwrap_or_else(|| {
        #[allow(clippy::cast_precision_loss)]
        let uniform = 1.0 / num_actions as f64;
        vec![uniform; num_actions]
    })
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::cfr::VanillaCfr;
    use crate::game::KuhnPoker;

    #[test]
    fn uniform_strategy_has_positive_exploitability() {
        let game = KuhnPoker::new();
        let strategy: HashMap<String, Vec<f64>> = HashMap::new();

        let exploitability = calculate_exploitability(&game, &strategy);

        assert!(
            exploitability > 0.0,
            "Uniform strategy should be exploitable"
        );
    }

    #[test]
    fn exploitability_decreases_with_training() {
        let game = KuhnPoker::new();
        let mut solver = VanillaCfr::new(game.clone());

        solver.train(100);
        let early_strategy = extract_strategy(&solver);
        let early_exploitability = calculate_exploitability(&game, &early_strategy);

        solver.train(9_900);
        let late_strategy = extract_strategy(&solver);
        let late_exploitability = calculate_exploitability(&game, &late_strategy);

        assert!(
            late_exploitability < early_exploitability,
            "Exploitability should decrease: early={early_exploitability}, late={late_exploitability}"
        );
    }

    #[test]
    fn well_trained_strategy_has_low_exploitability() {
        let game = KuhnPoker::new();
        let mut solver = VanillaCfr::new(game.clone());

        solver.train(50_000);
        let strategy = extract_strategy(&solver);
        let exploitability = calculate_exploitability(&game, &strategy);

        assert!(
            exploitability < 0.01,
            "Well-trained strategy should have low exploitability, got {exploitability}"
        );
    }

    fn extract_strategy(solver: &VanillaCfr<KuhnPoker>) -> HashMap<String, Vec<f64>> {
        let info_sets = [
            "J", "Q", "K", "Jc", "Qc", "Kc", "Jb", "Qb", "Kb", "Jcb", "Qcb", "Kcb",
        ];

        info_sets
            .iter()
            .filter_map(|&is| solver.get_average_strategy(is).map(|s| (is.to_string(), s)))
            .collect()
    }
}
